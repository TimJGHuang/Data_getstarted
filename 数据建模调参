
Task4-建模调参
主要模型分为线性回归模型和非线形回归模型：

线性回归模型

最普遍的形式是$f(x)=ax+b$，其中x向量代表一条样本{x1,x2,x3....xn}，其中x1，x2，x3代表样本的各个特征，w是一条向量代表了每个特征所占的权重，b是一个标量代表特征都为0时的预测值。困难的地方在于我们如何获得w和b这两个向量，在李航老师的统计学习方法中把一个学习过程分为了三部分，模型、策略、算法，为了获得w和b我们需要制定一定的策略，而这个策略在机器学习的领域中，往往描述为真实值与回归值的偏差。$loss=(f(x)-y)^2$ 我们希望的是能够减少在测试集上的预测值f(x)与真实值y的差别，从而获得一个最佳的权重参数，因此这里采用最小二乘估计。

决策树模型

GBDT模型

XGBoost模型

LightGBM模型

主要步骤如下：

step1 读取数据

step2 建模与评估

五折交叉验证:在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据。而训练集和评估集则牵涉到下面的知识了。因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）

step3 模型调参

贪心算法
对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。

网格调参
GridSearchCV：一种调参的方法，当你算法模型效果不是很好时，可以通过该方法来调整参数，通过循环遍历，尝试每一种参数组合，返回最好的得分值的参数组合

贝叶斯调参
贝叶斯优化通过基于目标函数的过去评估结果建立替代函数（概率模型），来找到最小化目标函数的值。贝叶斯方法与随机或网格搜索的不同之处在于，它在尝试下一组超参数时，会参考之前的评估结果，因此可以省去很多无用功。

